# Research Result for chatgpt
Advanced Bot Detection & Evasion: A Defensive and Analytical Study
Executive Summary

The bot–human “arms race” on the web is intensifying. Recent analyses estimate that over 50% of web traffic is generated by bots, with many of those attempts being malicious. To combat automated abuse (credential stuffing, scalping, scraping, etc.), specialized anti-bot systems now employ multi-layered fingerprinting and behavioral analysis. Nonetheless, studies show that the majority of protected sites remain vulnerable: simple Python/Selenium bots are blocked on ~75% of sites, but by using less-common setups (e.g. Safari via AppleScript or Chrome on Android) attackers bypass ~80% of these defenses. This report surveys the cutting-edge techniques used to detect headless and scripted browsers (Playwright, Puppeteer, Selenium) and the corresponding evasion methods that try to fool them. We focus on modern fingerprinting (as of 2025–26) and compare open-source stealth tools against commercial anti-bot solutions, all from a defensive security perspective.
Detection Pillars

Websites and CDNs scan for telltale signs that “a browser is being controlled by automation”. Common detection signals include:

    User-Agent and Headers: Headless Chrome often includes the string “HeadlessChrome” in its UA. Detecting scripts may also rely on atypical Accept-Language or Referer headers. In practice, simply checking navigator.userAgent for “Headless” can flag bots.

    navigator.webdriver: By default, automated Chromium (via Selenium/DevTools) sets navigator.webdriver == true. This JavaScript-accessible property is one of the easiest fingerprints; many libraries explicitly override or delete it in stealth mode.

    Chrome-specific APIs: In headless mode, certain objects like window.chrome or extension APIs may be missing. For example, non-headless Chrome defines a window.chrome namespace, whereas headless Chrome often does not.

    Browser Features: Headless browsers may lack plugins or report inconsistent permissions. For instance, the Plugins array is empty (navigator.plugins.length === 0) in headless mode, whereas a real Chrome has built-in plugins (PDF viewer, etc.). Similarly, querying Notification.permission may return contradictory values in headless mode. Another test: navigator.languages is typically an empty string or array in headless Chrome, unlike a normal browser (['en-US','en'], etc.).

    Canvas/WebGL Fingerprints: Automated engines sometimes produce subtle differences in HTML5 canvas or WebGL rendering. Anti-bot scripts draw a hidden canvas or WebGL scene and analyze the pixel or shader output. Automation may yield slightly different results or missing fonts/graphics, revealing its presence.

    Runtime Artifacts: Tools like Selenium/Chromedriver leave behind global variables. For example, Selenium’s internals create properties like window.cdc_adoQ... in the page’s JavaScript context. A recent GitHub issue notes that these window.cdc_* properties are a dead giveaway for ChromeDriver; removing them is required to avoid detection (Playwright has no such variables by design).

    Behavioral/Timing Anomalies: Apart from static properties, detectors also watch how browsers make network requests, load assets, or handle events. For example, CDP-driven bots may load resources without normal Referer headers or have perfectly regular mouse/timing patterns. While not traditional “fingerprint” fields, these operational differences are increasingly monitored.

In summary, detection relies on any discrepancy between a claimed browser identity and observable signals. Tests typically combine many signals – e.g., if (UA contains “HeadlessChrome**or**navigator.webdriver===true**or**navigator.plugins.length===0**or**navigator.languages===""**etc.**, **then** flag as automated):contentReference[oaicite:19]{index=19}:contentReference[oaicite:20]{index=20}. Anti-bot vendors (Cloudflare, Datadome, Akamai, etc.) feed these signals into ML models or rule sets. For example, one framework notes that **“common signals include**navigator.webdriver`, missing/spoofed features, and inconsistencies in surfaces such as canvas, WebGL, and device memory”.
Evasion Mechanics

Fundamental principle: Evasion tools aim to spoof or neutralize the above signals so the browser appears “normal.” Most techniques inject scripts or use browser APIs to override exposed properties and handshake behaviors. For instance, the popular puppeteer-extra-plugin-stealth automatically applies many patches behind the scenes. Its documentation confirms that it “fixes” various navigator fields: navigator.hardwareConcurrency, navigator.languages, navigator.plugins, navigator.vendor, and removes navigator.webdriver. It also overrides media codecs, WebGL vendor/renderer strings, and even the user-agent if needed. In practice, using stealth may look like:

const puppeteer = require('puppeteer-extra');
const StealthPlugin = require('puppeteer-extra-plugin-stealth');
puppeteer.use(StealthPlugin());
const browser = await puppeteer.launch({ headless: true });
// This launch creates a browser instance with patched properties:
// - navigator.webdriver is set to false/undefined
// - navigator.languages returns e.g. ['en-US','en']
// - navigator.plugins is populated with typical plugins
// - window.chrome and other chrome props are mocked

Playwright has a similar port. For example, one can use playwright-extra with playwright-extra-plugin-stealth, or manually do:

const { chromium } = require('playwright-extra');
const StealthPlugin = require('playwright-extra-plugin-stealth');
chromium.use(StealthPlugin());
const browser = await chromium.launch();
// Stealth plugin for Playwright adjusts the same set of fields.

Beyond these plugins, evasion at the engine level may involve:

    Navigator Hacks: Use page.evaluateOnNewDocument() (or Selenium’s execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument",…)) to redefine getters. For example, patch HTMLCanvasElement.prototype.toDataURL to add slight noise to the pixel hash, or replace the navigator getters via a JavaScript Proxy so they return chosen values. A deep-dive article shows how Puppeteer-stealth wraps navigator.languages with a Proxy to preserve the “native” appearance while returning spoofed values. Similarly, one can override navigator.hardwareConcurrency to mimic a typical CPU core count (e.g. 4 or 8) instead of the default (often 2) in headless mode.

    Canvas/WebGL Noise: Tools like CanvasBlocker or CanvasDefender work at the rendering level. They might introduce one random pixel difference in every canvas draw call so that the fingerprint is not stable. Alternatively, they can simply block the toDataURL() call entirely, yielding a blank or garbled canvas to thwart extraction. A cautionary note: overly aggressive overrides (e.g. removing all plugins or completely disabling canvas) can break site functionality, so stealth systems carefully whitelist or mimic reasonable defaults.

    Header and Transport Tweaks: On the network side, evasion may involve matching the TLS and HTTP protocol fingerprint of a real browser. For TLS, attackers sometimes use libraries like PolyTLS or custom TLS stacks to rotate cipher suites in a browser-like way. For HTTP/2, one might ensure the SETTINGS frame parameters match Chrome’s defaults (see next section). Some proxies (like curl-impersonate) directly adopt Chrome/Firefox TLS and HTTP2 stacks.

    Protocol-Level Avoidance: A new generation of “anti-detect” frameworks (e.g. nodriver or selenium-driverless) avoid using the Chrome DevTools Protocol (CDP) or WebDriver entirely. By driving the browser through OS-level inputs or rewiring WebDriver internals, they aim to avoid low-level artifacts. For example, selenium-driverless reimplements Selenium bindings to avoid sending the “CDP session start” signals that leave telltale traces. These approaches recognize that patching JS alone can create side effects, so they restructure the automation channel itself.

Key parameters & pitfalls: Every patch comes with trade-offs. For instance, overriding navigator.permissions to hide the “AutomationControlled” feature may prevent certain site features (like notifications) from working. Patching too many fields can be counterproductive: as one analysis notes, altering navigator.webdriver or navigator.languages is “now less useful” and can introduce new anomalies. The unified headless Chrome codebase (since Nov 2022) also means fewer differences exist by default – old tricks based on “missing APIs” often fail now. A further pitfall is maintainability: future browser updates may add new properties (the infamous window.cdc_… was one of these) that automation tools must catch. Thus stealth frameworks constantly update their patches to match the evolving browser engine.
Network Intelligence (TLS and HTTP/2/3 Fingerprinting)

Fundamental principle: Beyond client-side signals, automation can also be detected at the network layer. One powerful method is TLS fingerprinting. In a TLS handshake, the browser’s ClientHello message includes its supported cipher suites, TLS versions, extensions (and on HTTP/2, its SETTINGS frames). These vary between platforms and libraries. Tools like JA3 (2017, Salesforce) hash the ordered list of cipher suites and extensions to form a fingerprint. However, modern browsers randomized extensions order (Chrome 2023), making raw JA3 inconsistent across sessions.

To overcome this, JA4 (2023, FoxIO) was introduced as a next-gen TLS fingerprint. It normalizes handshake fields (e.g. sorts extensions) and adds new dimensions like ALPN (HTTP protocol hints). JA4 also comes as part of a suite (JA4+, including HTTP/2 (JA4H) and HTTP/3/QUIC fingerprints) that Cloudflare and others are adopting. By capturing TLS (and next-layer) metadata, these fingerprints identify the “stack” of the client: its OS, browser version, and even hardware clues.

Industry usage: DataDome describes TLS fingerprinting as a “solid method in the arsenal” of bot detection. Their ML models look for two patterns: (1) unique TLS fingerprints tied to known bots; and (2) inconsistencies between claimed browser identity and TLS traits. For example, if a client claims “Chrome/102 on Windows” but its TLS cipher list matches iOS Chrome, the mismatch flags it as fraudulent. In practice DataDome reports catching an attack by noting exactly such an inconsistency (TLS cipher suites implied iOS while the session was not iOS). Similarly, Cloudflare Bot Management gathers JA4 fingerprints at the edge: administrators can see what percentage of traffic with each JA4 is from known browsers vs known bots.

Akamai researchers observed bots actively tampering with TLS to avoid detection. So-called “cipher stunting” (randomizing the ClientHello) exploded in 2018–19, yielding billions of distinct fingerprints. In one report, the count of observed TLS fingerprint signatures went from tens of thousands to millions to billions in months as attackers randomized their cipher suites. Despite this, Akamai could still profile the attacker: the randomization patterns tied back to a specific Java-based tool. This highlights the cat-and-mouse nature: even TLS fingerprinting is not foolproof, but it remains a high-fidelity signal.

HTTP/2 and HTTP/3: The successor protocols add more fingerprinting data. HTTP/2’s frame sequence (SETTINGS, WINDOW_UPDATE, HEADERS, etc.) can differ by client. For instance, browsers vary the default SETTINGS parameters (header table size, window size, etc.) and the order of pseudo-headers (:method, :authority, etc.). Akamai has formalized an HTTP/2 fingerprint (often hashed as [SETTINGS]|WINDOW_UPDATE|PRIORITY|Pseudo-Header-Order|…) that can instantly reveal common HTTP/2 libraries. Scrapers using curl or Python’s h2 library, for example, produce fixed SETTINGS values that differ from Chrome’s defaults. Automated clients also often enable HTTP/2 without configuring its flow-control the same way a browser does. Thus, like TLS, HTTP/2 is very effective at spotting non-browsers. Cloudflare’s upcoming JA4+ includes JA4H to encompass these HTTP/2/3 features.

Detection techniques: In practice, systems combine TLS and HTTP signals. Common methods include: blacklisting known library fingerprints (e.g. curl/nghttp2, Python’s h2, Go’s net/http2 defaults); cross-checking header fingerprints against TLS (e.g. see if the ALPN and cipher suites match the claimed User-Agent); and looking for outdated/fixed configs (browsers update HTTP/2 settings over versions, so an old constant value is suspicious). For example, Chrome 94 might use SETTINGS HEADER_TABLE_SIZE=65536, MAX_CONCURRENT_STREAMS=1000, whereas curl’s default is quite different, immediately revealing a bot. Libraries like curl-impersonate or http2-client try to solve this by mimicking real browser settings. In summary, TLS/HTTP fingerprinting is a powerful layer of network intelligence that complements client-side evasion. (Polylibrary proxies and customized TLS stacks may help bots spoof these as well, but building a fully matching handshake is nontrivial.)
Comparative Review: Open-Source vs Commercial Solutions

Open-source tools and frameworks have led the charge in fingerprint evasion. The puppeteer-extra-plugin-stealth (and its Playwright counterpart) are early and popular: they hook into Chromium to mask many typical fingerprints. Other community projects include selenium-undetected-chromedriver for Python, and anti-detect browsers like Camoufox, Rebrowser (patches for Puppeteer/Playwright), Patchright, etc. Tabletop projects such as nodriver (Python) and Selenium-Driverless (Python) have emerged to avoid the very protocols (CDP/WebDriver) that reveal automation. Indeed, a 2025 survey lists tools like nodriver (no CDP usage) and selenium-driverless (bypassing WebDriver flags) as state-of-the-art alternatives. In essence, open frameworks either patch the browser APIs (masking navigator.webdriver, spoofing navigator.plugins, injecting canvas noise, etc.) or replace them with less-detectable methods.

Commercial anti-bot solutions (DataDome, Cloudflare Bot Management, Akamai Bot Manager, F5/BotD, etc.) take a broader approach. They aggregate signals at scale (global JA4/HTTP fingerprints, IP reputation, behavioral patterns) and feed them to ML engines. For example, DataDome’s documentation explicitly describes a “trust score” built from back-end signals (IP quality, TLS fingerprint, HTTP headers) and front-end signals (mouse movement, keystroke timing, and even a new invisible "device check" CAPTCHA). They even note that consistency is key: one approach they use is to detect when front-end and network fingerprints disagree (e.g. UA says Chrome on Windows but TLS matches Linux).

Studies comparing open and commercial approaches confirm that neither is foolproof. In one experiment, advanced evasive bots (using stealth techniques) still only achieved about 53% evasion success against DataDome and ~45% against F5’s BotD. This means roughly half of those “stealthed” attempts were caught. By analyzing these failures, researchers found that each anti-bot system looks at different fingerprint components: the bots that got through DataDome were not the same ones that evaded BotD. In other words, both solutions are effective in different ways. Generally, commercial systems win when they have large datasets: for example, >75% of sites block default Python/PhantomJS bots, but creative setups (like Android devices or custom browsers) can bypass ~80% of sites. That said, open-source tools continue to close the gap: new versions of stealth libraries and headless engines aim to match real browser fingerprints (indeed, headless Chrome now largely is real Chrome under the hood).

A practical takeaway: Puppeteer-extra-stealth (JS) and Playwright-stealth now handle most old detection flags (webdriver, languages, etc.) automatically. They will thwart basic bot checks. However, highly-resourced sites also use TLS/HTTP and behavioral analytics, which these plugins do not address. For those, solutions often involve “no-CDP” browsers (e.g. Nodriver) or resident proxies that mimic full browser stacks (e.g. Chrome on real hardware via SeleniumBase).
Security & Ethics

All research in this domain must be conducted responsibly. The fingerprint data we manipulate can be personal data under regulations. The EU’s GDPR considers browser fingerprinting as personal data when used to track users. By extension, the Turkish KVKK (Law 6698) similarly restricts the processing of unique user identifiers. This means that testing or deploying fingerprint evasion should be done with caution: always in controlled environments, with consent, and in compliance with privacy laws. Tools that alter fingerprints (especially on real users’ browsers) could violate data protection rules unless justified (for example, legitimate security testing or fraud prevention with user awareness).

Ethically, one must also honor website policies: obey robots.txt, terms of service, and anti-circumvention laws. Our focus here is on defensive insight and academic understanding, not on illicit scraping or denial-of-service. When publishing results or tools, it’s good practice to avoid giving instructions for criminal activity. For example, we describe how to modify TLS handshakes or override JS APIs, but we do not provide turnkey exploit scripts for unlawful access.

Finally, remember that over-reliance on spoofing can backfire: injecting inconsistencies or random delays can both tip off detectors and potentially harm legitimate users. The goal should be to blend in while remaining within legal and ethical bounds. As the EFF reminds us, any covert fingerprinting used for tracking must be transparent or consented to. Similarly, researchers should anonymize any real traffic fingerprints they use (e.g. don’t publish someone’s actual device JA4). Compliance (GDPR/KVKK) requires treating fingerprint attributes as sensitive personal data if they can identify a device or user. In summary: remain transparent about the research purpose, seek consent when needed, and stay on the side of privacy protection even while studying these detection techniques.
